<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Examples &amp; Use Cases - Crunchy Containers for PostgreSQL</title><meta http-equiv="Content-Type" content="text/html;charset=utf-8"></meta><meta property="og:site_name" content="Crunchy Containers"></meta><meta property="og:title" content="Examples &amp; Use Cases - Crunchy Containers for PostgreSQL"></meta><meta property="og:type" content="website"></meta><meta property="og:image:type" content="image/png"></meta><meta property="og:image" content="{[backrest-url-base]}/logo.svg"></meta><meta name="description" content="Examples &amp; Use Cases - Crunchy Containers for PostgreSQL"></meta><meta property="og:description" content="Examples &amp; Use Cases - Crunchy Containers for PostgreSQL"></meta><link rel="stylesheet" href="default.css" type="text/css"></link></head><body><div id="header-wrapper"><div class="page-header" id="page-header"><div class="page-header-logo"><img src="logo.svg"></div><div class="page-header-title">Examples & Use Cases - Crunchy Containers for PostgreSQL</div></div><div class="page-menu"><div class="menu-body"><div class="menu"><a class="menu-link" href="install.html">Home</a></div><div class="menu"><a class="menu-link" href="containers.html">Containers</a></div><div class="menu"><a class="menu-link" href="examples.html">Examples</a></div><div class="menu"><a class="menu-link" href="pitr.html">PITR</a></div><div class="menu"><a class="menu-link" href="dedicated.html">Dedicated</a></div><div class="menu"><a class="menu-link" href="backrest.html">Backup</a></div></div></div></div><div id="content-wrapper"><div id="sidebar-wrapper"><div id="sidebar"><div class="page-toc"><div class="page-toc-header"><div class="page-toc-title"></div></div><div class="page-toc-body"><div class="section1-toc"><div class="section1-toc-number">1</div><div class="section1-toc-title"><a href="#_usage">Usage</a></div><div class="section2-toc"><div class="section2-toc-number">1.1</div><div class="section2-toc-title"><a href="#_usage/_docker_customized_configuration">Docker Customized Configuration</a></div></div></div><div class="section1-toc"><div class="section1-toc-number">2</div><div class="section1-toc-title"><a href="#_container_examples">Container Examples</a></div><div class="section2-toc"><div class="section2-toc-number">2.1</div><div class="section2-toc-title"><a href="#_container_examples/_running_a_single_database">Running a Single Database</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.2</div><div class="section2-toc-title"><a href="#_container_examples/_creating_a_primary_database_with_pvc">Creating a Primary Database with PVC</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.3</div><div class="section2-toc-title"><a href="#_container_examples/_creating_a_primary_replica_database_cluster">Creating a Primary / Replica Database Cluster</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.4</div><div class="section2-toc-title"><a href="#_container_examples/_primary_replica_deployment">Primary / Replica Deployment</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.5</div><div class="section2-toc-title"><a href="#_container_examples/_automated_failover">Automated Failover</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.6</div><div class="section2-toc-title"><a href="#_container_examples/_performing_a_backup_amp_restore">Performing a Backup & Restore</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.7</div><div class="section2-toc-title"><a href="#_container_examples/_pgbackrest">pgbackrest</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.8</div><div class="section2-toc-title"><a href="#_container_examples/_pgadmin4_http">pgadmin4-http</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.9</div><div class="section2-toc-title"><a href="#_container_examples/_pgadmin4_https">pgadmin4-https</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.10</div><div class="section2-toc-title"><a href="#_container_examples/_pgpool">pgpool</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.11</div><div class="section2-toc-title"><a href="#_container_examples/_pgbadger">pgbadger</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.12</div><div class="section2-toc-title"><a href="#_container_examples/_postgres_gis">postgres-gis</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.13</div><div class="section2-toc-title"><a href="#_container_examples/_metrics_collection">Metrics Collection</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.14</div><div class="section2-toc-title"><a href="#_container_examples/_vacuum">Vacuum</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.15</div><div class="section2-toc-title"><a href="#_container_examples/_cron_scheduler">Cron Scheduler</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.16</div><div class="section2-toc-title"><a href="#_container_examples/_pgbouncer">pgbouncer</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.17</div><div class="section2-toc-title"><a href="#_container_examples/_synchronous_replication">Synchronous Replication</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.18</div><div class="section2-toc-title"><a href="#_container_examples/_statefulsets">Statefulsets</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.19</div><div class="section2-toc-title"><a href="#_container_examples/_pitr_pitr_point_in_time_recovery">pitr - PITR (point in time recovery)</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.20</div><div class="section2-toc-title"><a href="#_container_examples/_pg_audit">pg_audit</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.21</div><div class="section2-toc-title"><a href="#_container_examples/_docker_swarm">Docker Swarm</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.22</div><div class="section2-toc-title"><a href="#_container_examples/_pg_upgrade">pg_upgrade</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.23</div><div class="section2-toc-title"><a href="#_container_examples/_performing_a_pg_dump">Performing a pg_dump</a></div></div><div class="section2-toc"><div class="section2-toc-number">2.24</div><div class="section2-toc-title"><a href="#_container_examples/_performing_a_pg_restore">Performing a pg_restore</a></div></div></div><div class="section1-toc"><div class="section1-toc-number">3</div><div class="section1-toc-title"><a href="#_legal_notices">Legal Notices</a></div></div></div></div></div></div><div id="page-wrapper"><div id="page-body"><div class="section1"><a id="_usage"></a><div class="section1-header"><div class="section1-number">1</div><div class="section1-title">Usage</div></div><div class="section-body"><div class="section-body-text">Here are some useful resources for finding the right commands to troubleshoot & modify containers in the various environments shown in this guide:</div><ul class="list-unordered"><li class="list-unordered"><a href="http://www.bogotobogo.com/DevOps/Docker/Docker-Cheat-Sheet.php">Docker Cheat Sheet</a></li><li class="list-unordered"><a href="https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/">Kubectl Cheat Sheet</a></li><li class="list-unordered"><a href="https://github.com/nekop/openshift-sandbox/blob/master/docs/command-cheatsheet.md">OpenShift Cheat Sheet</a></li><li class="list-unordered"><a href="https://github.com/kubernetes/helm/blob/master/docs/using_helm.md">Helm Cheat Sheet</a></li></ul><div class="section2"><a id="_usage/_docker_customized_configuration"></a><div class="section2-header"><div class="section2-number">1.1</div><div class="section2-title">Docker Customized Configuration</div></div><div class="section-body"><div class="section-body-text">You can use your own version of the setup.sql SQL file to customize the initialization of database data and objects when the container and database are created.</div><div class="section-body-text">This example can be run as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/custom-setup
./run.sh</pre><div class="section-body-text">This works by placing a file named, setup.sql, within the /pgconf mounted volume directory. Portions of the setup.sql file are required for the crunchy container to work, see comments within the sample setup.sql file.</div><div class="section3"><a id="_usage/_docker_customized_configuration/_custom_configuration"></a><div class="section3-header"><div class="section3-number">1.1.1</div><div class="section3-title">Custom Configuration</div></div><div class="section-body"><div class="section-body-text">This example shows how you can use your own customized version of setup.sql when creating a PostgreSQL database container in Kubernetes.</div><div class="section-body-text">If you mount a /pgconf volume, crunchy-postgres will look at that directory for postgresql.conf, pg_hba.conf, and setup.sql. If it finds one of them it will use that file instead of the default files. Currently, if you specify a postgresql.conf file, you also need to specify a pg_hba.conf file.</div><div class="section-body-text">The example shows how a custom setup.sql file can be used. Run it as follows:</div><pre class="code-block">cd $CCPROOT/examples/kubernetes/custom-config
./run.sh</pre><div class="section-body-text">This will start a database container that will use an NFS mounted /pgconf directory that will contain the custom setup.sql file found in the example directory.</div></div></div><div class="section3"><a id="_usage/_docker_customized_configuration/_custom_configuration_2"></a><div class="section3-header"><div class="section3-number">1.1.2</div><div class="section3-title">Custom Configuration</div></div><div class="section-body"><div class="section-body-text">This example shows how you can use your own customized version of setup.sql when creating a PostgreSQL database container in OpenShift.</div><div class="section-body-text">If you mount a /pgconf volume, crunchy-postgres will look at that directory for postgresql.conf, pg_hba.conf, and setup.sql. If it finds one of them it will use that file instead of the default files. Currently, if you specify a postgresql.conf file, you also need to specify a pg_hba.conf file.</div><div class="section-body-text">The example shows how a custom setup.sql file can be used. Run it as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/custom-config
./run.sh</pre><div class="section-body-text">This will start a database container that will use an NFS mounted /pgconf directory that will contain the custom setup.sql file found in the example directory.</div><div class="section4"><a id="_usage/_docker_customized_configuration/_custom_configuration_2/_customized_configuration_with_synchronous_replica"></a><div class="section4-header"><div class="section4-number">1.1.2.1</div><div class="section4-title">Customized Configuration with Synchronous Replica</div></div><div class="section-body"><div class="section-body-text">This example shows how you can use your own customized version of postgresql.conf and pg_hba.conf to override the default configuration. It also specifies a synchronous replica in the postgresql.conf and starts it up upon creation.</div><div class="section-body-text">Run it as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/custom-config-sync
./run.sh</pre><div class="section-body-text">This will start a <b>csprimary</b> container that will use the custom config files when the database is running. It will also create a synchronous replica named <b>cssyncreplica</b>. This replica is then connected to the primary via streaming replication.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_custom_configuration_2/_configmap_database_credentials"></a><div class="section4-header"><div class="section4-number">1.1.2.2</div><div class="section4-title">Configmap Database Credentials</div></div><div class="section-body"><div class="section-body-text">This example shows how to use a configmap to store the postgresql.conf and pg_hba.conf files to be used when overriding the default configuration within the container.</div><div class="section-body-text">Start by running the database container:</div><pre class="code-block">cd $CCPROOT/examples/openshift/configmap
./run.sh</pre><div class="section-body-text">The files pg_hba.conf and postgresql.conf in the example directory are used to create a configmap object within OpenShift. Within the run.sh script, the configmap is created. Notice within the configmap.json file how the /pgconf mount is related to the configmap.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_custom_configuration_2/_templates_configuration"></a><div class="section4-header"><div class="section4-number">1.1.2.3</div><div class="section4-title">Templates Configuration</div></div><div class="section-body"><div class="section-body-text">An example of using OpenShift Templates to build pods, routes, services, etc can be found in the following directory:</div><pre class="code-block">$CCPROOT/examples/openshift/workshop</pre><div class="section-body-text">You use the <b>oc new-app</b> command to create objects from the JSON templates. This is an alternative way to create OpenShift objects instead of using <b>oc create</b>.</div><div class="section-body-text">See the README file within the workshop directory for instructions on running the example.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_custom_configuration_2/_secrets"></a><div class="section4-header"><div class="section4-number">1.1.2.4</div><div class="section4-title">Secrets</div></div><div class="section-body"><div class="section-body-text">You can use Kubernetes Secrets to set and maintain your database credentials. Secrets requires you base64 encode your user and password values as follows:</div><pre class="code-block">echo -n 'myuserid' | base64</pre><div class="section-body-text">You will paste these values into your JSON secrets files for values.</div><div class="section-body-text">This example allows you to set the PostgreSQL passwords using Kubernetes Secrets.</div><div class="section-body-text">The secret uses a base64 encoded string to represent the values to be read by the container during initialization. The encoded password value is <b>password</b>. Run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/secret
./run.sh</pre><div class="section-body-text">The secrets are mounted in the <b>/pguser</b>, <b>/pgprimary</b>, <b>/pgroot</b> volumes within the container and read during initialization. The container scripts create a PostgreSQL user with those values, and sets the passwords for the primary user and PostgreSQL superuser using the mounted secret volumes.</div><div class="section-body-text">When using secrets, you do NOT have to specify the following environment variables if you specify all three secrets volumes:</div><ul class="list-unordered"><li class="list-unordered">PG_USER</li><li class="list-unordered">PG_PASSWORD</li><li class="list-unordered">PG_ROOT_PASSWORD</li><li class="list-unordered">PG_PRIMARY_USER</li><li class="list-unordered">PG_PRIMARY_PASSWORD</li></ul><div class="section-body-text">You can test the container as follows, in all cases, the password is <b>password</b>:</div><pre class="code-block">psql -h secret-pg -U pguser1 postgres
psql -h secret-pg -U postgres postgres
psql -h secret-pg -U primary postgres</pre><div class="section-body-text">Secrets requires you base64 encode your user and password values as follows:</div><pre class="code-block">echo -n 'myuserid' | base64</pre><div class="section-body-text">You can paste these values into your JSON secrets files for values.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_custom_configuration_2/_ssl_authentication"></a><div class="section4-header"><div class="section4-number">1.1.2.5</div><div class="section4-title">SSL Authentication</div></div><div class="section-body"><div class="section-body-text">This example shows how you can configure PostgreSQL to use SSL for client authentication.</div><div class="section-body-text">The example requires SSL keys to be created and the example script <b>keys.sh</b> is required to be executed to create the required server and client certificates. This script also creates a client key configuration you can use to test with.</div><div class="section-body-text">The example requires an NFS volume, /pgconf, be mounted into which the PostgreSQL configuration files and keys are copied to. Permissions of the keys are important as well, they will need to be owned by either the <b>root</b> or <b>postgres</b> user. The <b>run.sh</b> script copies the required files and sets these permissions when executing the example.</div><div class="section-body-text">The <b>keys.sh</b> script creates a client cert with the <b>testuser</b> specified as the CN. The <b>testuser</b> PostgreSQL user is created by the <b>setup.sql</b> configuration script as normal. It is with the <b>testuser</b> role that you will test with.</div><div class="section-body-text">Run the PostgreSQL example as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/customer-config-ssl
./run.sh</pre><div class="section-body-text">A required step to make this example work is to define in your <b>/etc/hosts</b> file an entry that maps <b>server.crunchydata.com</b> to the example's service IP address, this is because we generate a server certificate with the server name of <b>server.crunchyhdata.com</b>.</div><div class="section-body-text">For example, if your service has an address as follows:</div><pre class="code-block"> oc get service
NAME                CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE
custom-config-ssl   172.30.211.108   <none>        5432/TCP</pre><div class="section-body-text">Then your <b>/etc/hosts</b> file needs an entry like this:</div><pre class="code-block">172.30.211.108 server.crunchydata.com</pre><div class="section-body-text">For a production Openshift installation, you'll likely want DNS names to resolve to the PostgreSQL Service name and generate server certificates using the DNS names instead of an example name like <b>server.crunchydata.com</b>.</div><div class="section-body-text">Once the container starts up, you can test the SSL connection as follows:</div><pre class="code-block">psql -h server.crunchydata.com -U testuser userdb</pre><div class="section-body-text">You should see a connection that looks like the following:</div><pre class="code-block">psql (9.6.8)
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

userdb=></pre></div></div></div></div><div class="section3"><a id="_usage/_docker_customized_configuration/_tips"></a><div class="section3-header"><div class="section3-number">1.1.3</div><div class="section3-title">Tips</div></div><div class="section-body"><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_postgresql_passwords"></a><div class="section4-header"><div class="section4-number">1.1.3.1</div><div class="section4-title">PostgreSQL Passwords</div></div><div class="section-body"><div class="section-body-text">The passwords used for the PostgreSQL user accounts are generated by the OpenShift <i>process</i> command. To inspect what value was supplied, you can inspect the primary pod as follows:</div><pre class="code-block">oc get pod pr-primary -o json | grep PG</pre><div class="section-body-text">Look for the values of the environment variables:</div><ul class="list-unordered"><li class="list-unordered">PG_USER</li><li class="list-unordered">PG_PASSWORD</li><li class="list-unordered">PG_DATABASE</li></ul></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_password_management"></a><div class="section4-header"><div class="section4-number">1.1.3.2</div><div class="section4-title">Password Management</div></div><div class="section-body"><div class="section-body-text">When you backup a database, the original user IDs and password credentials are copied over from the original database and saved. Because of this, you cannot use generated passwords as the new passwords will not be the same as the passwords stored in the backup.</div><div class="section-body-text">You have various options to deal with managing your passwords:</div><ul class="list-unordered"><li class="list-unordered">externalize your passwords using secrets instead of using generated values</li><li class="list-unordered">manually update your passwords to your known values after a restore</li></ul><div class="admonition"><div class="note">NOTE:</div><div class="note-text">Environment variables can be modified when there is a a deployment controller in use. Currently, only the replicas have a deployment controller in order to avoid the possibility of creating multiple primaries.</div></div><pre class="code-block">oc env dc/pg-primary-rc PG_PRIMARY_PASSWORD=foo PG_PRIMARY=user1</pre></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_examine_backup_logs"></a><div class="section4-header"><div class="section4-number">1.1.3.3</div><div class="section4-title">Examine Backup Logs</div></div><div class="section-body"><div class="section-body-text">Database backups are implemented as a Kubernetes Job. These are meant to run one time only and not be restarted by Kubernetes. To view jobs in OpenShift you enter:</div><pre class="code-block">oc get jobs
oc describe job backupjob</pre><div class="section-body-text">You can get detailed logs by referring to the pod identifier in the job <i>describe</i> output as follows:</div><pre class="code-block">oc logs backupjob-pxh2o</pre></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_backups"></a><div class="section4-header"><div class="section4-number">1.1.3.4</div><div class="section4-title">Backups</div></div><div class="section-body"><div class="section-body-text">Backups require the use of network storage like NFS in OpenShift. There is a required order of using NFS volumes in the manner we do database backups.</div><div class="section-body-text">There is a one-to-one relationship between a PV (persistent volume) and a PVC (persistence volume claim). You can NOT have a one-to-many relationship between PV and PVC(s).</div><div class="section-body-text">So, to do a database backup repeatedly, this general pattern will need to be followed.</div><ul class="list-unordered"><li class="list-unordered">as OpenShift admin user, create a unique PV (e.g. backup-pv-mydatabase)</li><li class="list-unordered">as a project user, create a unique PVC (e.g. backup-pvc-mydatabase)</li><li class="list-unordered">reference the unique PVC within the backup-job template</li><li class="list-unordered">execute the backup job template</li><li class="list-unordered">as a project user, delete the job</li><li class="list-unordered">as a project user, delete the PVC</li><li class="list-unordered">as OpenShift admin user, delete the unique PV</li></ul><div class="section-body-text">This procedure will need to be scripted and executed by the devops team when performing a database backup.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_restores"></a><div class="section4-header"><div class="section4-number">1.1.3.5</div><div class="section4-title">Restores</div></div><div class="section-body"><div class="section-body-text">To perform a database restore, we do the following:</div><ul class="list-unordered"><li class="list-unordered">locate the NFS path to the database backup we want to restore with</li><li class="list-unordered">edit a PV to use that NFS path</li><li class="list-unordered">edit a PV to specify a unique label</li><li class="list-unordered">create the PV</li><li class="list-unordered">edit a PVC to use the previously created PV, specifying the same label used in the PV</li><li class="list-unordered">edit a database template, specifying the PVC to be used for mounting to the /backup directory in the database pod</li><li class="list-unordered">create the database pod</li></ul><div class="section-body-text">If the /pgdata directory is blank AND the /backup directory contains a valid PostgreSQL backup, it is assumed the user wants to perform a database restore.</div><div class="section-body-text">The restore logic will copy /backup files to /pgdata before starting the database. It will take time for the copying of the files to occur since this might be a large amount of data and the volumes might be on slow networks. You can view the logs of the database pod to measure the copy progress.</div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_log_aggregation"></a><div class="section4-header"><div class="section4-number">1.1.3.6</div><div class="section4-title">Log Aggregation</div></div><div class="section-body"><div class="section-body-text">OpenShift can be configured to include the EFK stack for log aggregation. OpenShift Administrators can configure the EFK stack as documented here:</div><div class="section-body-text"><a href="https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html">https://docs.openshift.com/enterprise/3.1/install_config/aggregate_logging.html</a></div></div></div><div class="section4"><a id="_usage/_docker_customized_configuration/_tips/_nss_wrapper"></a><div class="section4-header"><div class="section4-number">1.1.3.7</div><div class="section4-title">nss_wrapper</div></div><div class="section-body"><div class="section-body-text">If an OpenShift deployment requires that random generated UIDs be supported by containers, the Crunchy containers can be modified similar to those located here to support the use of nss_wrapper to equate the random generated UIDs/GIDs by OpenShift with the postgres user.</div><div class="section-body-text"><a href="https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh">https://github.com/openshift/postgresql/blob/master/9.4/root/usr/share/container-scripts/postgresql/common.sh</a></div></div></div></div></div></div></div></div></div><div class="section1"><a id="_container_examples"></a><div class="section1-header"><div class="section1-number">2</div><div class="section1-title">Container Examples</div></div><div class="section-body"><div class="section2"><a id="_container_examples/_running_a_single_database"></a><div class="section2-header"><div class="section2-number">2.1</div><div class="section2-title">Running a Single Database</div></div><div class="section-body"><div class="section-body-text">This example starts a single PostgreSQL container and service, the most simple of examples.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">For all environments, the script additionally creates:</div><ul class="list-unordered"><li class="list-unordered">A persistent volume claim</li><li class="list-unordered">A container named <b>basic</b></li><li class="list-unordered">The database using predefined environment variables</li></ul><div class="section-body-text">And specifically for the Kubernetes and OpenShift environments:</div><ul class="list-unordered"><li class="list-unordered">A pod named <b>basic</b></li><li class="list-unordered">A service named <b>basic</b></li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_running_a_single_database/_docker"></a><div class="section3-header"><div class="section3-number">2.1.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To create the example and run the container:</div><pre class="code-block">cd $CCPROOT/examples/docker/basic
./run.sh</pre><div class="section-body-text">Connect from your local host as follows:</div><pre class="code-block">psql -h localhost -p 12000 -U testuser -W userdb</pre></div></div><div class="section3"><a id="_container_examples/_running_a_single_database/_kubernetes"></a><div class="section3-header"><div class="section3-number">2.1.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">To create the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/basic
./run.sh</pre><div class="section-body-text">Connect from your local host as follows:</div><pre class="code-block">psql -h basic -U postgres postgres</pre></div></div><div class="section3"><a id="_container_examples/_running_a_single_database/_helm"></a><div class="section3-header"><div class="section3-number">2.1.3</div><div class="section3-title">Helm</div></div><div class="section-body"><div class="section-body-text">This example resides under the $CCPROOT/examples/helm directory. View the README to run this example using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/basic/README.md">here</a>.</div></div></div><div class="section3"><a id="_container_examples/_running_a_single_database/_openshift"></a><div class="section3-header"><div class="section3-number">2.1.4</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">To create the example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/basic
./run.sh</pre><div class="section-body-text">Connect from your local host as follows:</div><pre class="code-block">psql -h basic.openshift.svc.cluster.local -U testuser userdb</pre></div></div></div></div><div class="section2"><a id="_container_examples/_creating_a_primary_database_with_pvc"></a><div class="section2-header"><div class="section2-number">2.2</div><div class="section2-title">Creating a Primary Database with PVC</div></div><div class="section-body"><div class="section-body-text">The other example <b>basic</b> uses emptyDir volumes for persistence; if it is desired to create a PVC based volume to store the PostgreSQL data files for a single primary pod, run the following example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-pvc
./run.sh</pre></div></div><div class="section2"><a id="_container_examples/_creating_a_primary_replica_database_cluster"></a><div class="section2-header"><div class="section2-number">2.3</div><div class="section2-title">Creating a Primary / Replica Database Cluster</div></div><div class="section-body"><div class="section-body-text">This example starts a primary and a replica pod containing a PostgreSQL database.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">For the Docker environment, the script additionally creates:</div><ul class="list-unordered"><li class="list-unordered">A docker volume using the local driver for the primary</li><li class="list-unordered">A docker volume using the local driver for the replica</li><li class="list-unordered">A container named <b>primary</b> binding to port 12007</li><li class="list-unordered">A container named <b>replica</b> binding to port 12008</li><li class="list-unordered">A mapping of the PostgreSQL port 5432 within the container to the localhost port 12000</li><li class="list-unordered">The database using predefined environment variables</li></ul><div class="section-body-text">And specifically for the Kubernetes and OpenShift environments:</div><ul class="list-unordered"><li class="list-unordered">emptyDir volumes for persistence</li><li class="list-unordered">A pod named <b>primary</b></li><li class="list-unordered">A pod named <b>replica</b></li><li class="list-unordered">A service named <b>primary</b></li><li class="list-unordered">A service named <b>replica</b></li><li class="list-unordered">The database using predefined environment variables</li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_creating_a_primary_replica_database_cluster/_docker_2"></a><div class="section3-header"><div class="section3-number">2.3.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To create the example and run the container:</div><pre class="code-block">cd $CCPROOT/examples/docker/primary-replica
./run.sh</pre><div class="section-body-text">Connect from your local host as follows:</div><pre class="code-block">psql -h localhost -p 12007 -U testuser -W userdb
psql -h localhost -p 12008 -U testuser -W userdb</pre></div></div><div class="section3"><a id="_container_examples/_creating_a_primary_replica_database_cluster/_docker_compose"></a><div class="section3-header"><div class="section3-number">2.3.2</div><div class="section3-title">Docker-Compose</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/compose/primary-replica
docker-compose up</pre><div class="section-body-text">To deploy more than one replica, run the following:</div><pre class="code-block">docker-compose up --scale db-replica=3</pre><div class="section-body-text">To psql into the created database containers, first identify the ports exposed on the containers:</div><pre class="code-block">docker ps</pre><div class="section-body-text">Next, using psql, connect to the service:</div><pre class="code-block">psql -d userdb -h 0.0.0.0 -p <CONTAINER_PORT> -U testuser</pre><div class="admonition"><div class="note">NOTE:</div><div class="note-text">See <b>PG_PASSWORD</b> in <b>docker-compose.yml</b> for the user password.</div></div><div class="section-body-text">To tear down the example, run the following:</div><pre class="code-block">docker-compose stop
docker-compose rm</pre></div></div><div class="section3"><a id="_container_examples/_creating_a_primary_replica_database_cluster/_kubernetes_2"></a><div class="section3-header"><div class="section3-number">2.3.3</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Run the following command to deploy a primary and replica database cluster:</div><pre class="code-block">cd $CCPROOT/examples/kube/primary-replica
./run.sh</pre><div class="section-body-text">It takes about a minute for the replica to begin replicating with the primary. To test out replication, see if replication is underway with this command:</div><pre class="code-block">psql -h pr-primary -U postgres postgres -c 'table pg_stat_replication'</pre><div class="section-body-text">If you see a line returned from that query it means the primary is replicating to the replica. Try creating some data on the primary:</div><pre class="code-block">psql -h pr-primary -U postgres postgres -c 'create table foo (id int)'
psql -h pr-primary -U postgres postgres -c 'insert into foo values (1)'</pre><div class="section-body-text">Then verify that the data is replicated to the replica:</div><pre class="code-block">psql -h pr-replica -U postgres postgres -c 'table foo'</pre><div class="section-body-text"><b>primary-replica-dc</b></div><div class="section-body-text">If you wanted to experiment with scaling up the number of replicas, you can run the following example:</div><pre class="code-block">cd $CCPROOT/examples/kube/primary-replica-dc
./run.sh</pre><div class="section-body-text">You can verify that replication is working using the same commands as above.</div><div class="section-body-text">This example creates 2 replicas when it initially starts. To scale up the number of replicas and view what the deployment looks like before and after, run these commands:</div><pre class="code-block">kubectl get deployment
kubectl scale --current-replicas=2 --replicas=3 deployment/replica-dc
kubectl get deployment
kubectl get pod</pre><div class="section-body-text">You can verify that you now have 3 replicas by running this query on the primary:</div><pre class="code-block">psql -h primary-dc -U postgres postgres -c 'table pg_stat_replication'</pre></div></div><div class="section3"><a id="_container_examples/_creating_a_primary_replica_database_cluster/_helm_2"></a><div class="section3-header"><div class="section3-number">2.3.4</div><div class="section3-title">Helm</div></div><div class="section-body"><div class="section-body-text">This example resides under the $CCPROOT/examples/helm directory. View the README to run this example using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/primary-replica/README.md">here</a>.</div></div></div><div class="section3"><a id="_container_examples/_creating_a_primary_replica_database_cluster/_openshift_2"></a><div class="section3-header"><div class="section3-number">2.3.5</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the following command to deploy a primary and replica database cluster:</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-replica
./run.sh</pre><div class="section-body-text">You can then connect to the database instance as follows:</div><pre class="code-block">psql -h pr-primary -U testuser -W userdb</pre><div class="section-body-text"><b>primary-replica-dc</b></div><div class="section-body-text">The primary-replica example creates a primary and non-scaling replica; if you wanted to experiment with scaling replicas, try the primary-replica-dc example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-replica-dc
./run.sh</pre><div class="section-body-text">Connect to the PostgreSQL instances with the following:</div><pre class="code-block">psql -h primary-dc.pgproject.svc.cluster.local -U testuser userdb
psql -h replica-dc.pgproject.svc.cluster.local -U testuser userdb</pre><div class="section-body-text">Here is an example of increasing or scaling up the PostgreSQL <i>replica</i> pods to 2:</div><pre class="code-block">oc scale rc replica-dc-1 --replicas=2</pre><div class="section-body-text">Enter the following commands to verify the PostgreSQL replication is working:</div><pre class="code-block">psql -c 'table pg_stat_replication' -h primary-dc.pgproject.svc.cluster.local -U primary postgres
psql -h replica-dc.pgproject.svc.cluster.local -U primary postgres</pre><div class="section-body-text">The replica service is load balancing between multiple replicas; this can be shown by running this command multiple times and the IP address should alternate between the replicas:</div><pre class="code-block">psql -h replica-dc -U postgres postgres -c 'select inet_server_addr()'</pre><div class="section-body-text"><b>primary-replica-rc-pvc</b></div><div class="section-body-text">The previous primary-replica deployments used emptyDir volumes for persistence. This example uses a PVC based volume in your NFS directory for the primary and the replicas.</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-replica-rc-pvc
./run.sh</pre><div class="section-body-text">Upon examining the configured NFS directory, the PostgreSQL data directories that are created and used by the primary and replica pods are visible. Testing the example uses the same commands as above, substituting the name <b>primary-replica-rc-pvc</b>.</div></div></div></div></div><div class="section2"><a id="_container_examples/_primary_replica_deployment"></a><div class="section2-header"><div class="section2-number">2.4</div><div class="section2-title">Primary / Replica Deployment</div></div><div class="section-body"><div class="section-body-text">Starting in release 1.2.8, the PostgreSQL container can accept an environment variable named PGDATA_PATH_OVERRIDE. If set, the /pgdata/subdir path will use a path subdir name of your choosing instead of the default which is the hostname of the container.</div><div class="section-body-text">This example shows how a Deployment of a PostgreSQL primary is supported. A pod is a deployment that uses a hostname generated by Kubernetes; because of this, a new hostname will be defined upon restart of the primary pod.</div><div class="section-body-text">For finding the /pgdata that pertains to the pod, you will need to specify a /pgdata/subdir name that never changes. This requirement is handled by the PGDATA_PATH_OVERRIDE environment variable.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">This example will create the following in your Kubernetes and OpenShift environments:</div><ul class="list-unordered"><li class="list-unordered">primary-dc service, uses a PVC to persist PostgreSQL data</li><li class="list-unordered">replica-dc service, uses emptyDir persistence</li><li class="list-unordered">primary-dc Deployment of replica count 1 for the primary PostgreSQL database pod</li><li class="list-unordered">replica-dc Deployment of replica count 1 for the replica</li><li class="list-unordered">replica2-dc Deployment of replica count 1 for the 2nd replica</li><li class="list-unordered">ConfigMap to hold a custom postgresql.conf, setup.sql, and pg_hba.conf files</li><li class="list-unordered">Secrets for the primary user, superuser, and normal user to hold the passwords</li><li class="list-unordered">Volume mount for /pgbackrest and /pgwal</li></ul><div class="section-body-text">The persisted data for the PostgreSQL primary is found under /pgdata/primary-dc. If you delete the primary pod, the Deployment will create another pod for the primary, and will be able to start up immediately since we are using the same /pgdata/primary-dc data directory.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_primary_replica_deployment/_kubernetes_3"></a><div class="section3-header"><div class="section3-number">2.4.1</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Start the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/primary-deployment
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_primary_replica_deployment/_openshift_3"></a><div class="section3-header"><div class="section3-number">2.4.2</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Start the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-deployment
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_automated_failover"></a><div class="section2-header"><div class="section2-number">2.5</div><div class="section2-title">Automated Failover</div></div><div class="section-body"><div class="section-body-text">This example shows how to run the crunchy-watch container to perform an automated failover. For the example to work, the host on which you are running needs to allow read-write access to /run/docker.sock. The crunchy-watch container runs as the <b>postgres</b> user, so adjust the file permissions of /run/docker.sock accordingly.</div><div class="section-body-text">The <b>primary-replica</b> example is required to be run before this example.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_automated_failover/_docker_3"></a><div class="section3-header"><div class="section3-number">2.5.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/watch
./run.sh</pre><div class="section-body-text">This will start the watch container which tests every few seconds whether the primary database is running, if not, it will trigger a failover (using docker exec) on the replica host.</div><div class="section-body-text">Test it out by stopping the primary:</div><pre class="code-block">docker stop primary
docker logs watch</pre><div class="section-body-text">Look at the watch container logs to see it perform the failover.</div></div></div><div class="section3"><a id="_container_examples/_automated_failover/_kubernetes_4"></a><div class="section3-header"><div class="section3-number">2.5.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/watch
./run.sh</pre><div class="section-body-text">Check out the log of the watch container as follows:</div><pre class="code-block">kubectl log watch</pre><div class="section-body-text">Then trigger a failover using this command:</div><pre class="code-block">kubectl delete pod pr-primary</pre><div class="section-body-text">Resume watching the watch container's log and verify that it detects the primary is not reachable and performs a failover on the replica.</div><div class="section-body-text">A final test is to see if the old replica is now a fully functioning primary by inserting some test data into it as follows:</div><pre class="code-block">psql -h pr-primary -U postgres postgres -c 'create table failtest (id int)'</pre><div class="section-body-text">The above command still works because the watch container has changed the labels of the replica to make it a primary, so the primary service will still work and route now to the new primary even though the pod is named replica.</div><div class="section-body-text"><b>Tip</b></div><div class="section-body-text">You can view the labels on a pod with this command:</div><pre class="code-block">kubectl describe pod pr-replica | grep Label</pre></div></div><div class="section3"><a id="_container_examples/_automated_failover/_openshift_4"></a><div class="section3-header"><div class="section3-number">2.5.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">The following script will create an OpenShift service account which is used by the crunchy-watch container to perform the failover. Also, it will set policies that allow the service account the ability to edit resources within your namespace. Finally, it will create the container that will <i>watch</i> the PostgreSQL cluster.</div><pre class="code-block">cd $CCPROOT/examples/openshift/watch
./run.sh</pre><div class="section-body-text">At this point, the watcher will sleep every 20 seconds (configurable) to see if the primary is responding. If the primary doesn't respond, the watcher will perform the following logic:</div><ul class="list-unordered"><li class="list-unordered">log into OpenShift using the service account</li><li class="list-unordered">set its current project</li><li class="list-unordered">find the first replica pod</li><li class="list-unordered">delete the primary service saving off the primary service definition</li><li class="list-unordered">create the trigger file on the first replica pod</li><li class="list-unordered">wait 20 seconds for the failover to complete on the replica pod</li><li class="list-unordered">edit the replica pod's label to match that of the primary</li><li class="list-unordered">recreate the primary service using the stored service definition</li><li class="list-unordered">loop through the other remaining replica and delete its pod</li></ul><div class="section-body-text">At this point, clients when access the primary's service will actually be accessing the new primary. Also, OpenShift will recreate the number of replicas to its original configuration which each replica pointed to the new primary. Replication from the primary to the new replicas will be started as each new replica is started by OpenShift.</div><div class="section-body-text">To test it out, delete the primary pod and view the watch pod log:</div><pre class="code-block">oc delete pod pr-primary
oc logs watch
oc get pod</pre></div></div></div></div><div class="section2"><a id="_container_examples/_performing_a_backup_amp_restore"></a><div class="section2-header"><div class="section2-number">2.6</div><div class="section2-title">Performing a Backup & Restore</div></div><div class="section-body"><div class="section-body-text">The script assumes you are going to backup the <b>basic</b> container created in the first example, so you need to ensure that container is running. This example assumes you have configured NFS as described in the <a href="install.adoc">installation documentation</a>. Things to point out with this example include its use of persistent volumes and volume claims to store the backup data files to an NFS server.</div><div class="section-body-text">A successful backup will perform pg_basebackup on the <b>basic</b> container and store the backup in the NFS mounted volume under a directory named basic-backups. Each backup will be stored in a subdirectory with a timestamp as the name, allowing any number of backups to be kept.</div><div class="section-body-text">The backup script will do the following:</div><ul class="list-unordered"><li class="list-unordered">Start up a backup container named backup-job</li><li class="list-unordered">Run pg_basebackup on the container named basic</li><li class="list-unordered">Store the backup in /tmp/backups/basic-backups directory</li><li class="list-unordered">Exit after the backup</li></ul><div class="section-body-text">When you are ready to restore from the backup, the restore example runs a PostgreSQL container passing in the backup location. The startup of the container will use rsync to copy the backup data to this new container, and then launch PostgreSQL which will use the backup data to start.</div><div class="section-body-text">The restore script will do the following:</div><ul class="list-unordered"><li class="list-unordered">Start up a container named primary-restore</li><li class="list-unordered">Copy the backup files from the previous backup example into /pgdata</li><li class="list-unordered">Start up the container using the backup files</li><li class="list-unordered">Map the PostgreSQL port of 5432 in the container to your local host port of 12001</li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_performing_a_backup_amp_restore/_docker_4"></a><div class="section3-header"><div class="section3-number">2.6.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the backup with this command:</div><pre class="code-block">cd $CCPROOT/examples/docker/backup
./run.sh</pre><div class="section-body-text"><b>primary-restore</b></div><div class="section-body-text">When you are ready to restore from the backup created, run the following example:</div><pre class="code-block">cd $CCPROOT/examples/docker/restore
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_backup_amp_restore/_kubernetes_5"></a><div class="section3-header"><div class="section3-number">2.6.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/backup-job
./run.sh</pre><div class="section-body-text">The Kubernetes Job type executes a pod and then the pod exits. You can view the Job status using this command:</div><pre class="code-block">kubectl get job</pre><div class="section-body-text">You should find the backup archive in this location:</div><pre class="code-block">ls /mnt/nfsfileshare/basic-backups</pre><div class="section-body-text"><b>primary-restore</b></div><div class="section-body-text">When ready to restore, you will need the timestamped directory path under /mnt/nfsfileshare/basic-backups in order to locate the backup to use. Edit the primary-restore.json file and update the BACKUP_PATH setting to specify the NFS backup path you want to restore with.</div><pre class="code-block">"name": "BACKUP_PATH",
"value": "basic-backups/2016-05-27-14-35-33"</pre><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/primary-restore
./run.sh</pre><div class="section-body-text">Test the restored database as follows:</div><pre class="code-block">psql -h primary-restore -U postgres postgres</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_backup_amp_restore/_openshift_5"></a><div class="section3-header"><div class="section3-number">2.6.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Start the backup:</div><pre class="code-block">cd $CCPROOT/examples/openshift/backup-job
./run.sh</pre><div class="section-body-text">The <b>backup-job.json</b> file within that directory specifies a <b>persistentVolumeReclaimPolicy</b> of <b>Retain</b> to tell OpenShift that we want to keep the volume contents after the removal of the PV.</div><div class="section-body-text"><b>primary-restore</b></div><div class="section-body-text">When ready to restore, you will need the timestamped directory path under /mnt/nfsfileshare/basic-backups in order to locate the backup to use. Edit the primary-restore.json file and update the BACKUP_PATH setting to specify the NFS backup path you want to restore with.</div><pre class="code-block">"name": "BACKUP_PATH",
"value": "basic-backups/2016-05-27-14-35-33"</pre><div class="section-body-text">Then create the pod:</div><pre class="code-block">cd $CCPROOT/examples/openshift/primary-restore
./run.sh</pre><div class="section-body-text">When the database pod starts, it will copy the backup files to the database directory inside the pod and start up Postgres as usual.</div><div class="section-body-text">The restore only takes place if:</div><ul class="list-unordered"><li class="list-unordered">the /pgdata directory is empty</li><li class="list-unordered">the /backups directory contains a valid postgresql.conf file</li></ul></div></div></div></div><div class="section2"><a id="_container_examples/_pgbackrest"></a><div class="section2-header"><div class="section2-number">2.7</div><div class="section2-title">pgbackrest</div></div><div class="section-body"><div class="section-body-text">Starting in release 1.3.1, the <b>pgbackrest</b> utility has been added to the crunchy-postgres container. See the <a href="backrest.adoc">pgbackrest Documentation</a> for details on how this feature works within the Crunchy Container Suite.</div></div></div><div class="section2"><a id="_container_examples/_pgadmin4_http"></a><div class="section2-header"><div class="section2-number">2.8</div><div class="section2-title">pgadmin4-http</div></div><div class="section-body"><div class="section-body-text">This example deploys the pgadmin4 v2 web user interface for PostgreSQL without TLS.</div><div class="section-body-text">After running the example, you should be able to browse to <a href="http://127.0.0.1:5050">http://127.0.0.1:5050</a> and log into the web application using a user ID of <b>admin@admin.com</b> and password of <b>password</b>.</div><div class="section-body-text">If you are running this example using Kubernetes or OpenShift, replace <b>127.0.0.1:5050</b> with the <NODE_IP>:30000.</div><div class="section-body-text">To get the node IP, run the following:</div><pre class="code-block"># Kube
kubectl describe pod pgadmin4 | grep Node:

# OCP
oc describe pod pgadmin4 | grep Node:</pre><div class="section-body-text">See the <a href="http://pgadmin.org">pgadmin4 documentation</a> for more details.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pgadmin4_http/_docker_5"></a><div class="section3-header"><div class="section3-number">2.8.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To run this example, run the following:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgadmin4-http
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_pgadmin4_http/_kubernetes_6"></a><div class="section3-header"><div class="section3-number">2.8.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Start the container as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgadmin4-http
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_pgadmin4_http/_openshift_6"></a><div class="section3-header"><div class="section3-number">2.8.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">To run this example, run the following:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgadmin4-http
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_pgadmin4_https"></a><div class="section2-header"><div class="section2-number">2.9</div><div class="section2-title">pgadmin4-https</div></div><div class="section-body"><div class="section-body-text">This example deploys the pgadmin4 v2 web user interface for PostgreSQL with TLS.</div><div class="section-body-text">After running the example, you should be able to browse to <a href="https://127.0.0.1:5050">https://127.0.0.1:5050</a> and log into the web application using a user ID of <b>admin@admin.com</b> and password of <b>password</b>.</div><div class="section-body-text">If you are running this example using Kubernetes or OpenShift, replace <b>127.0.0.1:5050</b> with the <NODE_IP>:30000.</div><div class="section-body-text">To get the node IP, run the following:</div><pre class="code-block"># Kube
kubectl describe pod pgadmin4 | grep Node:

# OCP
oc describe pod pgadmin4 | grep Node:</pre><div class="section-body-text">See the <a href="http://pgadmin.org">pgadmin4 documentation</a> for more details.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pgadmin4_https/_docker_6"></a><div class="section3-header"><div class="section3-number">2.9.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To run this example, run the following:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgadmin4-https
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_pgadmin4_https/_kubernetes_7"></a><div class="section3-header"><div class="section3-number">2.9.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Start the container as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgadmin4-https
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_pgadmin4_https/_openshift_7"></a><div class="section3-header"><div class="section3-number">2.9.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">To run this example, run the following:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgadmin4-https
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_pgpool"></a><div class="section2-header"><div class="section2-number">2.10</div><div class="section2-title">pgpool</div></div><div class="section-body"><div class="section-body-text">An example is provided that will run a pgpool container in conjunction with the primary and replica example (<b>primary-replica</b>) provided above.</div><div class="section-body-text">You can execute both INSERT and SELECT statements after connecting to pgpool. The container will direct INSERT statements to the primary and SELECT statements will be sent round-robin to both primary and replica.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pgpool/_docker_7"></a><div class="section3-header"><div class="section3-number">2.10.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Create the container as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgpool
./run.sh</pre><div class="section-body-text">Enter the following command to connect to the pgpool container that is mapped to your local port 12003:</div><pre class="code-block">psql -h localhost -U testuser -p 12003 userdb</pre></div></div><div class="section3"><a id="_container_examples/_pgpool/_kubernetes_8"></a><div class="section3-header"><div class="section3-number">2.10.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/primary-replica
./run.sh
cd $CCPROOT/examples/kube/pgpool
./run.sh</pre><div class="section-body-text">The example is configured to allow the <b>testuser</b> to connect to the <b>userdb</b> database as follows:</div><pre class="code-block">psql -h pgpool -U testuser userdb</pre><div class="section-body-text">You can view the nodes that pgpool is configured for by running:</div><pre class="code-block">psql -h pgpool -U testuser userdb -c 'show pool_nodes'</pre></div></div><div class="section3"><a id="_container_examples/_pgpool/_openshift_8"></a><div class="section3-header"><div class="section3-number">2.10.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the following command to deploy the pgpool service:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgpool
./run.sh</pre><div class="section-body-text">Next, you can access the primary replica cluster via the pgpool service by entering the following command:</div><pre class="code-block">psql -h pgpool -U testuser userdb
psql -h pgpool -U testuser postgres</pre><div class="section-body-text">You can view the nodes that pgpool is configured for by running:</div><pre class="code-block">psql -h pgpool -U testuser userdb -c 'show pool_nodes'</pre></div></div></div></div><div class="section2"><a id="_container_examples/_pgbadger"></a><div class="section2-header"><div class="section2-number">2.11</div><div class="section2-title">pgbadger</div></div><div class="section-body"><div class="section-body-text">A pgbadger example is provided that will run a HTTP server that when invoked, will generate a pgbadger report on a given database.</div><div class="section-body-text">pgbadger reads the log files from a database to product an HTML report that shows various PostgreSQL statistics and graphs.</div><div class="section-body-text">The port utilized for this tool is port 14000 for Docker environments and port 10000 for Kubernetes and Openshift environments.</div><div class="section-body-text">Additional requirements to build this container include <b>golang</b>. On RHEL 7.2, golang is found in the <i>server optional</i> repository which needs to be enabled in order to install this dependency.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pgbadger/_docker_8"></a><div class="section3-header"><div class="section3-number">2.11.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To run the example: * modify the run-badger.sh script to refer to the Docker container that you want to run pgbadger against * refer to the container's data directory * start the container that you are referencing</div><div class="section-body-text">Then, run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/badger
./run.sh</pre><div class="section-body-text">After execution, the container will run and provide a simple HTTP command you can browse to view the report. As you run queries against the database, you can invoke this URL to generate updated reports:</div><pre class="code-block">curl http://127.0.0.1:14000/api/badgergenerate</pre></div></div><div class="section3"><a id="_container_examples/_pgbadger/_kubernetes_9"></a><div class="section3-header"><div class="section3-number">2.11.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/badger
./run.sh</pre><div class="section-body-text">After execution, the container will run and provide a simple HTTP command you can browse to view the report. As you run queries against the database, you can invoke this URL to generate updated reports:</div><pre class="code-block">curl http://badger:10000/api/badgergenerate</pre><div class="section-body-text"><b>Tip</b></div><div class="section-body-text">You can view the database container logs using this command:</div><pre class="code-block">kubectl logs badger-example -c badger</pre></div></div><div class="section3"><a id="_container_examples/_pgbadger/_openshift_9"></a><div class="section3-header"><div class="section3-number">2.11.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">To run the example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/badger
./run.sh</pre><div class="section-body-text">After execution, the container will run and provide a simple HTTP command you can browse to view the report. As you run queries against the database, you can invoke this URL to generate updated reports:</div><pre class="code-block">curl http://badger-example:10000/api/badgergenerate</pre><div class="section-body-text">You can view this output in a browser if you allow port forwarding from your container to your server host using a command like this:</div><pre class="code-block">socat tcp-listen:10001,reuseaddr,fork tcp:pg-primary:10000</pre><div class="section-body-text">This command maps port 10000 of the service/container to port 10001 of the local server. You can now use your browser to see the badger report.</div><div class="section-body-text">This is a short-cut to expose a service to the external world. OpenShift would normally configure a router in such a manner where you could <i>expose</i> the service in an OpenShift way.</div><div class="section-body-text">The official documentation for installing OpenShift on a router can be found <a href="https://docs.openshift.com/container-platform/3.6/install_config/router/index.html">here</a>.</div></div></div></div></div><div class="section2"><a id="_container_examples/_postgres_gis"></a><div class="section2-header"><div class="section2-number">2.12</div><div class="section2-title">postgres-gis</div></div><div class="section-body"><div class="section-body-text">An example is provided that will run a postgres-gis pod/service in Kubernetes/OpenShift and a container in Docker.</div><div class="section-body-text">The container creates a default database called <b>userdb</b>, a default user called <b>testuser</b> and a default password of <b>password</b>.</div><div class="section-body-text">You can view the extensions that postgres-gis has enabled by running the following command (postgis should be listed):</div><pre class="code-block">psql -h postgres-gis -U testuser userdb -c '\dx'</pre><div class="section-body-text">To validate that PostGIS is installed and which version is running, run the command:</div><pre class="code-block">psql -h postgres-gis -U testuser userdb -c "SELECT postgis_full_version();"</pre><div class="section-body-text">You should expect to see output similar to:</div><pre class="code-block">postgis_full_version
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 POSTGIS="2.4.2 r16113" PGSQL="100" GEOS="3.5.0-CAPI-1.9.0 r4084" PROJ="Rel. 4.8.0, 6 March 2012" GDAL="GDAL 1.11.4, released 2016/01/25" LIBXML="2.9.1" LIBJSON="0.11" TOPOLOGY RASTER
(1 row)</pre><div class="section-body-text">To exercise some of the basic PostGIS functionality for validation (in this case defining 2D geometry point - given inputs of longitude and latitude), run the command:</div><pre class="code-block">psql -h postgres-gis -U testuser userdb -c "select ST_MakePoint(28.385200,-81.563900);"</pre><div class="section-body-text">You should expect to see output similar to:</div><pre class="code-block">                st_makepoint
--------------------------------------------
 0101000000516B9A779C623C40B98D06F0166454C0
(1 row)</pre><div class="section-body-text">To shutdown the instance and remove the pod/container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_postgres_gis/_docker_9"></a><div class="section3-header"><div class="section3-number">2.12.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Create the container as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/postgres-gis
./run.sh</pre><div class="section-body-text">Enter the following command to connect to the postgres-gis container that is mapped to your local port 12000:</div><pre class="code-block">psql -h localhost -U testuser -p 12000 userdb</pre></div></div><div class="section3"><a id="_container_examples/_postgres_gis/_kubernetes_10"></a><div class="section3-header"><div class="section3-number">2.12.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/postgres-gis
./run.sh</pre><div class="section-body-text">The example is configured to allow the <b>testuser</b> to connect to the <b>userdb</b> database as follows:</div><pre class="code-block">psql -h postgres-gis -U testuser userdb</pre></div></div><div class="section3"><a id="_container_examples/_postgres_gis/_openshift_10"></a><div class="section3-header"><div class="section3-number">2.12.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the following command to deploy the postgres-gis pod and service:</div><pre class="code-block">cd $CCPROOT/examples/openshift/postgres-gis
./run.sh</pre><div class="section-body-text">Next, you can access the postgres-gis pod via the postgres-gis service by entering the following command:</div><pre class="code-block">psql -h postgres-gis -U testuser userdb
psql -h postgres-gis -U testuser postgres</pre></div></div></div></div><div class="section2"><a id="_container_examples/_metrics_collection"></a><div class="section2-header"><div class="section2-number">2.13</div><div class="section2-title">Metrics Collection</div></div><div class="section-body"><div class="section-body-text">You can collect various PostgreSQL metrics from your database container by running a crunchy-collect container that points to your database container.</div><div class="section-body-text">This will start up 3 containers and services:</div><ul class="list-unordered"><li class="list-unordered">Prometheus (<a href="http://crunchy-prometheus:9090">http://crunchy-prometheus:9090</a>)</li><li class="list-unordered">Prometheus gateway (<a href="http://crunchy-promgateway:9091">http://crunchy-promgateway:9091</a>)</li><li class="list-unordered">Grafana (<a href="http://crunchy-grafana:3000">http://crunchy-grafana:3000</a>)</li></ul><div class="section-body-text">Every 3 minutes the collection container will collect PostgreSQL metrics and push them to the crunchy-prometheus database. You can graph them using the crunchy-grafana container.</div><div class="section-body-text">If firewalld is enabled in your environment, it may be necessary to allow the necessary ports through the firewall. This can be accomplished by the following:</div><pre class="code-block">firewall-cmd --permanent --new-zone metrics
firewall-cmd --permanent --zone metrics --add-port 9090/tcp
firewall-cmd --permanent --zone metrics --add-port 9091/tcp
firewall-cmd --permanent --zone metrics --add-port 3000/tcp
firewall-cmd --reload</pre><div class="section-body-text">All metrics collected by this set of containers in addition to details on accessing the custom Grafana dashboards provided are fully described in this <a href="metrics.adoc">document.</a></div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_metrics_collection/_docker_10"></a><div class="section3-header"><div class="section3-number">2.13.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To start this set of containers, run the following:</div><pre class="code-block">cd $CCPROOT/examples/docker/metrics
./run.sh</pre><div class="section-body-text">An example has been provided that runs a database container in addition to the associated metrics collection container. Run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/collect
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_metrics_collection/_kubernetes_11"></a><div class="section3-header"><div class="section3-number">2.13.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/metrics
./run.sh</pre><div class="section-body-text">If you want your metrics and dashboards to persist to NFS, run this script:</div><pre class="code-block">cd $CCPROOT/examples/kube/metrics
./run-pvc.sh</pre><div class="section-body-text">This example runs a pod that includes a database container and a metrics collection container. A service is also created for the pod.</div><pre class="code-block">cd $CCPROOT/examples/kube/collect
./run.sh</pre><div class="section-body-text">You can view the collect container logs using this command:</div><pre class="code-block">kubectl logs -c collect primary-collect</pre><div class="section-body-text">You can access the database or drive load against it using this command:</div><pre class="code-block">psql -h primary-collect -U postgres postgres</pre></div></div><div class="section3"><a id="_container_examples/_metrics_collection/_openshift_11"></a><div class="section3-header"><div class="section3-number">2.13.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">First, create the crunchy-metrics pod which contains the Prometheus data store and the Grafana graphing web application:</div><pre class="code-block">cd $CCPROOT/examples/openshift/metrics
./run.sh</pre><div class="section-body-text">Next, start a PostgreSQL pod that has the crunchy-collect container as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/collect
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_vacuum"></a><div class="section2-header"><div class="section2-number">2.14</div><div class="section2-title">Vacuum</div></div><div class="section-body"><div class="section-body-text">You can perform a PostgreSQL vacuum command by running the crunchy-vacuum container. You specify a database to vacuum using environment variables. By default, it will specify the <b>basic</b> example; you will need to start the <b>basic</b> container before running <b>vacuum</b>.</div><div class="section-body-text">The crunchy-vacuum container image exists to allow a DBA a way to run a job either individually or scheduled to perform a variety of vacuum operations.</div><div class="section-body-text">This example performs a vacuum on a single table in the primary PostgreSQL database. The crunchy-vacuum image is executed, passed in the PostgreSQL connection parameters to the single-primary PostgreSQL container. The type of vacuum performed is dictated by the environment variables passed into the job. Vacuum is controlled via the following environment variables:</div><ul class="list-unordered"><li class="list-unordered">VAC_FULL - when set to true adds the FULL parameter to the VACUUM command</li><li class="list-unordered">VAC_TABLE - when set, allows you to specify a single table to vacuum, when not specified, the entire database tables are vacuumed</li><li class="list-unordered">JOB_HOST - required variable is the PostgreSQL host we connect to</li><li class="list-unordered">PG_USER - required variable is the PostgreSQL user we connect with</li><li class="list-unordered">PG_DATABASE - required variable is the PostgreSQL database we connect to</li><li class="list-unordered">PG_PASSWORD - required variable is the PostgreSQL user password we connect with</li><li class="list-unordered">PG_PORT - allows you to override the default value of 5432</li><li class="list-unordered">VAC_ANALYZE - when set to true adds the ANALYZE parameter to the VACUUM command</li><li class="list-unordered">VAC_VERBOSE - when set to true adds the VERBOSE parameter to the VACUUM command</li><li class="list-unordered">VAC_FREEZE - when set to true adds the FREEZE parameter to the VACUUM command</li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_vacuum/_docker_11"></a><div class="section3-header"><div class="section3-number">2.14.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/vacuum
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_vacuum/_kubernetes_12"></a><div class="section3-header"><div class="section3-number">2.14.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/vacuum-job/
./run.sh</pre><div class="section-body-text">Verify the job is completed:</div><pre class="code-block">kubectl get job</pre><div class="section-body-text">View the docker log of the vacuum job's pod:</div><pre class="code-block">docker logs $(docker ps -a | grep crunchy-vacuum | cut -f 1 -d' ')</pre></div></div><div class="section3"><a id="_container_examples/_vacuum/_openshift_12"></a><div class="section3-header"><div class="section3-number">2.14.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the example as follows:</div><pre class="code-block">cd ../vacuum-job
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_cron_scheduler"></a><div class="section2-header"><div class="section2-number">2.15</div><div class="section2-title">Cron Scheduler</div></div><div class="section-body"><div class="section-body-text">The crunchy-dba container implements a cron scheduler. The purpose of the crunchy-dba container is to offer a way to perform simple DBA tasks that occur on some form of schedule such as backup jobs or running a vacuum on a single PostgreSQL database container (such as the <b>basic</b> example).</div><div class="section-body-text">You can either run the crunchy-dba container as a single pod or include the container within a database pod.</div><div class="section-body-text">The crunchy-dba container makes use of a Service Account to perform the startup of scheduled jobs. The Kubernetes Job type is used to execute the scheduled jobs with a Restart policy of Never.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_cron_scheduler/_kubernetes_13"></a><div class="section3-header"><div class="section3-number">2.15.1</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">The script to schedule vacuum on a regular schedule is executed through the following commands:</div><pre class="code-block">cd $CCPROOT/examples/kube/dba
./run-vac.sh</pre><div class="section-body-text">To run the script for scheduled backups, run the following in the same directory:</div><pre class="code-block">./run-backup.sh</pre><div class="section-body-text">Individual parameters for both can be modified within their respective JSON files; please see <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/docs/containers.adoc">containers.adoc</a> for a full list of what can be modified.</div></div></div></div></div><div class="section2"><a id="_container_examples/_pgbouncer"></a><div class="section2-header"><div class="section2-number">2.16</div><div class="section2-title">pgbouncer</div></div><div class="section-body"><div class="section-body-text">The pgbouncer utility can be used to provide a connection pool to PostgreSQL databases.</div><div class="section-body-text">This example configures pgbouncer to provide connection pooling for the primary and pg-replica databases. It also sets the FAILOVER environment variable which will cause a failover to be triggered if the primary database can not be reached.</div><div class="section-body-text">After triggering the failover, pgbouncer will notice that the primary is not reachable and will touch the trigger file on the configured replica database to start the failover. The pgbouncer container will then reconfigure pgbouncer to relabel the replica database into the primary database so clients to pgbouncer will be able to connect to the primary as before the failover.</div><div class="section-body-text">This example is required to run in conjunction with another container, by default the <b>primary-replica</b> example.</div><div class="section-body-text">Additionally, the example assumes you have an NFS share path of /mnt/nfsfileshare/. NFS is required to mount the pgbouncer configuration files which are then mounted to /pgconf in the crunchy-pgbouncer container.</div><div class="section-body-text">If you mount a /pgconf volume, crunchy-postgres will look at that directory for postgresql.conf, pg_hba.conf, and setup.sql. If it finds one of them it will use that file instead of the default files.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pgbouncer/_docker_12"></a><div class="section3-header"><div class="section3-number">2.16.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">The pgbouncer example is run as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgbouncer
./run.sh</pre><div class="section-body-text">To trigger the failover, stop the primary database:</div><pre class="code-block">docker stop primary</pre><div class="section-body-text">To log into the database from the pgbouncer connection pool you would enter the following using the default password <b>password</b>:</div><pre class="code-block">psql -h localhost -p 12005 -U testuser primary</pre></div></div><div class="section3"><a id="_container_examples/_pgbouncer/_kubernetes_14"></a><div class="section3-header"><div class="section3-number">2.16.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgbouncer
./run.sh</pre><div class="section-body-text">Connect to the <b>primary</b> and <b>replica</b> databases as follows:</div><pre class="code-block">psql -h pgbouncer -U postgres primary
psql -h pgbouncer -U postgres replica</pre><div class="section-body-text">The names <b>primary</b> and <b>replica</b> are pgbouncer configured names and don't necessarily have to match the database name in the actual PostgreSQL instance.</div><div class="section-body-text">View the pgbouncer log as follows:</div><pre class="code-block">kubectl log pgbouncer</pre><div class="section-body-text">Next, test the failover capability within the crunchy-watch container using the following:</div><pre class="code-block">kubectl delete pod pr-primary</pre><div class="section-body-text">Take another look at the pgbouncer log and you will see it trigger the failover to the replica pod. After this failover you should be able to execute the command:</div><pre class="code-block">psql -h pgbouncer -U postgres primary</pre></div></div><div class="section3"><a id="_container_examples/_pgbouncer/_openshift_13"></a><div class="section3-header"><div class="section3-number">2.16.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgbouncer
./run.sh</pre><div class="section-body-text">Test the example by killing off the primary database container as follows:</div><pre class="code-block">oc delete pod pr-primary</pre><div class="section-body-text">Then watch the pgbouncer log as follows to confirm it detects the loss of the primary:</div><pre class="code-block">oc logs pgbouncer</pre><div class="section-body-text">After the failover is completed, you should be able to access the new primary using the primary service as follows:</div><pre class="code-block">psql -h pr-primary.openshift.svc.cluster.local -U primaryuser postgres</pre><div class="section-body-text">and access the replica as follows:</div><pre class="code-block">psql -h pr-replica.openshift.svc.cluster.local -U primaryuser postgres</pre><div class="section-body-text">or via the pgbouncer proxy as follows:</div><pre class="code-block">psql -h pgbouncer.openshift.svc.cluster.local  -U primaryuser primary</pre></div></div></div></div><div class="section2"><a id="_container_examples/_synchronous_replication"></a><div class="section2-header"><div class="section2-number">2.17</div><div class="section2-title">Synchronous Replication</div></div><div class="section-body"><div class="section-body-text">This example deploys a PostgreSQL cluster with a primary, a synchronous replica, and an asynchronous replica. The two replicas share the same Service.</div><div class="section-body-text">Connect to the <b>primarysync</b> and <b>replicasync</b> databases as follows for both the Kubernetes and OpenShift environments:</div><pre class="code-block">psql -h primarysync -U postgres postgres -c 'create table test (id int)'
psql -h primarysync -U postgres postgres -c 'insert into test values (1)'
psql -h primarysync -U postgres postgres -c 'table pg_stat_replication'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'
psql -h replicasync -U postgres postgres -c 'select inet_server_addr(), * from test'</pre><div class="section-body-text">This set of queries will show you the IP address of the PostgreSQL replica container. Notice the changing IP address due to the round-robin service proxy being used for both replicas. The example queries also show that both replicas are replicating from the primary.</div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_synchronous_replication/_docker_13"></a><div class="section3-header"><div class="section3-number">2.17.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">To run this example, run the following:</div><pre class="code-block">cd $CCPROOT/examples/docker/sync
./run.sh</pre><div class="section-body-text">You can test the replication status on the primary by using the following command and the password <q>password</q>:</div><pre class="code-block">psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'table pg_stat_replication'</pre><div class="section-body-text">You should see 2 rows, 1 for the async replica and 1 for the sync replica. The sync_state column shows values of async or sync.</div><div class="section-body-text">You can test replication to the replicas by entering some data on the primary like this, and then querying the replicas for that data:</div><pre class="code-block">psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'create table foo (id int)'
psql -h 127.0.0.1 -p 12010 -U postgres postgres -c 'insert into foo values (1)'
psql -h 127.0.0.1 -p 12011 -U postgres postgres -c 'table foo'
psql -h 127.0.0.1 -p 12012 -U postgres postgres -c 'table foo'</pre></div></div><div class="section3"><a id="_container_examples/_synchronous_replication/_kubernetes_15"></a><div class="section3-header"><div class="section3-number">2.17.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/sync
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_synchronous_replication/_openshift_14"></a><div class="section3-header"><div class="section3-number">2.17.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/sync
./run.sh</pre></div></div></div></div><div class="section2"><a id="_container_examples/_statefulsets"></a><div class="section2-header"><div class="section2-number">2.18</div><div class="section2-title">Statefulsets</div></div><div class="section-body"><div class="section-body-text">This example deploys a statefulset named <b>pgset</b>. The statefulset is a new feature in Kubernetes as of version 1.5 and in OpenShift Origin as of version 3.5. Statefulsets have replaced PetSets going forward.</div><div class="section-body-text">This example creates 2 PostgreSQL containers to form the set. At startup, each container will examine its hostname to determine if it is the first container within the set of containers.</div><div class="section-body-text">The first container is determined by the hostname suffix assigned by Kubernetes to the pod. This is an ordinal value starting with <b>0</b>.</div><div class="section-body-text">If a container sees that it has an ordinal value of <b>0</b>, it will update the container labels to add a new label of:</div><pre class="code-block">name=$PG_PRIMARY_HOST</pre><div class="section-body-text">In this example, PG_PRIMARY_HOST is specified as <b>pgset-primary</b>.</div><div class="section-body-text">By default, the containers specify a value of <b>name=pgset-replica</b>.</div><div class="section-body-text">There are 2 services that end user applications will use to access the PostgreSQL cluster, one service (pgset-primary) routes to the primary container and the other (pgset-replica) to the replica containers.</div><pre class="code-block">$ kubectl get service
NAME            CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kubernetes      10.96.0.1       <none>        443/TCP    22h
pgset           None            <none>        5432/TCP   1h
pgset-primary    10.97.168.138   <none>        5432/TCP   1h
pgset-replica   10.97.218.221   <none>        5432/TCP   1h</pre><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_statefulsets/_kubernetes_16"></a><div class="section3-header"><div class="section3-number">2.18.1</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Start the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/statefulset
./run.sh</pre><div class="section-body-text">You can access the primary database as follows:</div><pre class="code-block">psql -h pgset-primary -U postgres postgres</pre><div class="section-body-text">You can access the replica databases as follows:</div><pre class="code-block">psql -h pgset-replica -U postgres postgres</pre><div class="section-body-text">You can scale the number of containers using this command, this will essentially create an additional replica databse:</div><pre class="code-block">kubectl scale pgset --replica=3</pre><div class="section4"><a id="_container_examples/_statefulsets/_kubernetes_16/_statefulset_using_dynamic_provisioning"></a><div class="section4-header"><div class="section4-number">2.18.1.1</div><div class="section4-title">Statefulset using Dynamic Provisioning</div></div><div class="section-body"><div class="section-body-text">The example in <b>examples/statefulset-dyn</b> is almost an exact copy of the previous statefulset example; however, this example uses Dynamic Storage Provisioning to automatically create Persistent Volume Claims based on StorageClasses. This Kubernetes feature is available on Google Container Engine which this example was tested upon.</div><div class="section-body-text">You can run the example as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/statefulset-dyn
./run.sh</pre><div class="section-body-text">This will create a StorageClass named <b>slow</b> which you can view using:</div><pre class="code-block">kubectl get storageclass
NAME      TYPE
slow      kubernetes.io/gce-pd</pre><div class="section-body-text">The example causes Kube to create the required PVCs automatically:</div><pre class="code-block">kubectl get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
pgdata-pgset-0   Bound     pvc-06334f6f-371b-11e7-9bda-42010a8000e9   1Gi        RWX           slow           5m
pgdata-pgset-1   Bound     pvc-063795b3-371b-11e7-9bda-42010a8000e9   1Gi        RWX           slow           5m</pre><div class="section-body-text">More information on dynamic storage provisioning can be found here: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></div></div></div></div></div><div class="section3"><a id="_container_examples/_statefulsets/_helm_3"></a><div class="section3-header"><div class="section3-number">2.18.2</div><div class="section3-title">Helm</div></div><div class="section-body"><div class="section-body-text">This example resides under the $CCPROOT/examples/helm directory. View the README to run this example using Helm <a href="https://github.com/CrunchyData/crunchy-containers/blob/master/examples/helm/statefulset/README.md">here</a>.</div></div></div><div class="section3"><a id="_container_examples/_statefulsets/_openshift_15"></a><div class="section3-header"><div class="section3-number">2.18.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Build the example:</div><pre class="code-block">cd $CCPROOT/examples/openshift/statefulset
./run.sh</pre><div class="section-body-text">This will create a statefulset named pgset, which will create 2 pods, pgset-0 and pgset-1:</div><pre class="code-block">oc get statefulset
oc get pod</pre><div class="section-body-text">A service is created for the primary and another service for the replica:</div><pre class="code-block">oc get service</pre><div class="section-body-text">The statefulset ordinal value of 0 is used to determine which pod will act as the PostgreSQL primary, all other ordinal values will assume the replica role.</div></div></div></div></div><div class="section2"><a id="_container_examples/_pitr_pitr_point_in_time_recovery"></a><div class="section2-header"><div class="section2-number">2.19</div><div class="section2-title">pitr - PITR (point in time recovery)</div></div><div class="section-body"><div class="section-body-text">This is an example of performing point in time recovery on your database. See the <a href="pitr.adoc">PITR Documentation</a> for details on PITR concepts and how PITR is implemented within the Suite.</div><div class="section-body-text">It takes about 1 minute for the database to become ready for use after initially starting.</div><div class="section-body-text">This database is created with the ARCHIVE_MODE and ARCHIVE_TIMEOUT environment variables set.</div><div class="admonition"><div class="warning">WARNING:</div><div class="warning-text">WAL segment files are written to the /tmp directory. Leaving the example running for a long time could fill up your /tmp directory.</div></div><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_pitr_pitr_point_in_time_recovery/_docker_14"></a><div class="section3-header"><div class="section3-number">2.19.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Create a database container as follows:</div><pre class="code-block">cd $CCPROOT/examples/docker/pitr
./run-primary-pitr.sh</pre><div class="section-body-text">Next, we will create a base backup of that database using this:</div><pre class="code-block">./run-primary-pitr-backup.sh</pre><div class="section-body-text">After creating the base backup of the database, WAL segment files are created every 60 seconds that contain any database changes. These segments are stored in the /tmp/primary-data/master-wal directory.</div><div class="section-body-text">Create some data in your database using this command:</div><pre class="code-block">psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('beforechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c 'create table pitrtest (id int)'
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('afterchanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "select pg_create_restore_point('nomorechanges')"
psql -h 127.0.0.1 -p 12000 -U postgres postgres -c "checkpoint"</pre><div class="section-body-text">Next, stop the database to avoid conflicts with the WAL files while attempting to do a restore from them:</div><pre class="code-block">docker stop primary-pitr</pre><div class="section-body-text">The commands above set restore point labels which we can use to mark the points in the recovery process we want to reference when creating our restored database. Points before and after the test table were made.</div><div class="section-body-text">Next, let's edit the restore script to use the base backup files created in the step above. You can view the backup path name under the /tmp/backups/primary-pitr-backups/ directory. You will see another directory inside of this path with a name similar to <b>2016-09-21-21-03-29</b>. Copy and paste that value into the run-restore-pitr.sh script in the <b>BACKUP</b> environment variable.</div><div class="section-body-text">In order to restore the database before we created test table in the last command, you'll need uncomment to the RECOVERY_TARGET_NAME label <b>-e RECOVERY_TARGET_NAME=beforechanges</b> to define the restore target name. After that, run the script.</div><pre class="code-block">vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre><div class="section-body-text">The WAL segments are read and applied when restoring from the database backup. At this point, you should be able to verify that the database was restored to the point before creating the test table:</div><pre class="code-block">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'</pre><div class="section-body-text">This SQL command should show that the pitrtest table does not exist at this recovery time. The output should be similar to:</div><div class="section-body-text">PostgreSQL allows you to pause the recovery process if the target name or time is specified. This pause would allow a DBA a chance to review the recovery time/name and see if this is what they want or expect. If so, the DBA can run the following command to resume and complete the recovery:</div><pre class="code-block">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'</pre><div class="section-body-text">Until you run the statement above, the database will be left in read-only mode.</div><div class="section-body-text">Next, run the script to restore the database to the <b>afterchanges</b> restore point, do this by updating the RECOVERY_TARGET_NAME to <b>afterchanges</b>:</div><pre class="code-block">vi ./run-restore-pitr.sh
./run-restore-pitr.sh</pre><div class="section-body-text">After this restore, you should be able to see the test table:</div><pre class="code-block">psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'select pg_xlog_replay_resume()'</pre><div class="section-body-text">Lastly, start a recovery using all of the WAL files. This will get the restored database as current as possible. To do so, edit the script to remove the RECOVERY_TARGET_NAME environment setting completely:</div><pre class="code-block">./run-restore-pitr.sh
sleep 30
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'table pitrtest'
psql -h 127.0.0.1 -p 12001 -U postgres postgres -c 'create table foo (id int)'</pre><div class="section-body-text">At this point, you should be able to create new data in the restored database and the test table should be present. When you recover the entire WAL history, resuming the recovery is not necessary to enable writes.</div></div></div><div class="section3"><a id="_container_examples/_pitr_pitr_point_in_time_recovery/_kubernetes_17"></a><div class="section3-header"><div class="section3-number">2.19.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">This example is identical to the OpenShift PITR example; please see below for details on how the PITR example works.</div><div class="section-body-text">The only differences are the following:</div><ul class="list-unordered"><li class="list-unordered">paths are <b>$CCPROOT/examples/kube/pitr</b></li><li class="list-unordered">JSON and scripts are modified to work with Kubernetes</li><li class="list-unordered"><b>kubectl</b> commands are used instead of <b>oc</b> commands</li><li class="list-unordered">database services resolve to <b>default.svc.cluster.local</b> instead of <b>openshift.svc.cluster.local</b></li></ul></div></div><div class="section3"><a id="_container_examples/_pitr_pitr_point_in_time_recovery/_openshift_16"></a><div class="section3-header"><div class="section3-number">2.19.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Start by running the example database container:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pitr
./run-primary-pitr.sh</pre><div class="section-body-text">This step will create a database container, <b>primary-pitr</b>. This container is configured to continuously write WAL segment files to a mounted volume (/pgwal).</div><div class="section-body-text">After you start the database, you will create a base backup using this command:</div><pre class="code-block">./run-primary-pitr-backup.sh</pre><div class="section-body-text">This will create a backup and write the backup files to a persistent volume (/pgbackup).</div><div class="section-body-text">Next, create some recovery targets within the database by running the SQL commands against the <b>primary-pitr</b> database as follows:</div><pre class="code-block">./run-sql.sh</pre><div class="section-body-text">This will create recovery targets named <b>beforechanges</b>, <b>afterchanges</b>, and <b>nomorechanges</b>. It will create a table, <b>pitrtest</b>, between the <b>beforechanges</b> and <b>afterchanges</b> targets. It will also run a SQL CHECKPOINT to flush out the changes to WAL segments.</div><div class="section-body-text">Next, now that we have a base backup and a set of WAL files containing our database changes, we can shut down the <b>primary-pitr</b> database to simulate a database failure. Do this by running the following:</div><pre class="code-block">oc delete pod primary-pitr</pre><div class="section-body-text">Next, we will create 3 different restored database containers based upon the base backup and the saved WAL files.</div><div class="section-body-text">First, we restore prior to the <b>beforechanges</b> recovery target. This recovery point is <b>before</b> the <b>pitrtest</b> table is created.</div><div class="section-body-text">Edit the primary-pitr-restore.json file, and edit the environment variable to indicate we want to use the <b>beforechanges</b> recovery point:</div><pre class="code-block">}, {
"name": "RECOVERY_TARGET_NAME",
"value": "beforechanges"
}, {</pre><div class="section-body-text">Then run the following to create the restored database container:</div><pre class="code-block">./run-restore-pitr.sh</pre><div class="section-body-text">After the database has restored, you should be able to perform a test to see if the recovery worked as expected:</div><pre class="code-block">psql -h primary-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'table pitrtest'
psql -h primary-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'create table foo (id int)'
psql -h primary-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'select pg_xlog_replay_resume()'
psql -h primary-pitr-restore.openshift.svc.cluster.local -U postgres postgres -c 'create table foo (id int)'</pre><div class="section-body-text">The output of these command should show that the <b>pitrtest</b> table is not present. It should also show that you can not create a new table because the database is paused in recovery mode. Lastly, if you execute a <b>resume</b> command, it will show that you can now create a table as the database has fully recovered.</div><div class="section-body-text">You can also test that if <b>afterchanges</b> is specified, that the <b>pitrtest</b> table is present but that the database is still in recovery mode.</div><div class="section-body-text">Lastly, you can test a full recovery using <b>all</b> of the WAL files, if you remove the <b>RECOVERY_TARGET_NAME</b> environment variable completely.</div><div class="section-body-text">The NFS portions of this example depend upon an NFS file system with the following path configurations be present:</div><pre class="code-block">/mnt/nfsfileshare
/mnt/nfsfileshare/backups
/mnt/nfsfileshare/WAL</pre><div class="section-body-text">Once you recover a database using PITR, it will be in read-only mode. To make the database resume as a writable database, run the following sql command:</div><pre class="code-block">select pg_xlog_replay_resume();</pre><div class="section-body-text">This command changed for PG10 to:</div><pre class="code-block">postgres=# select pg_wal_replay_resume();</pre></div></div></div></div><div class="section2"><a id="_container_examples/_pg_audit"></a><div class="section2-header"><div class="section2-number">2.20</div><div class="section2-title">pg_audit</div></div><div class="section-body"><div class="section-body-text">This example provides an example of enabling pg_audit output. As of release 1.3, pg_audit is included in the crunchy-postgres container and is added to the PostgreSQL shared library list in the postgresql.conf.</div><div class="section-body-text">Given the numerous ways pg_audit can be configured, the exact pg_audit configuration is left to the user to define. pg_audit allows you to configure auditing rules either in postgresql.conf or within your SQL script.</div><div class="section-body-text">For this test, we place pg_audit statements within a SQL script and verify that auditing is enabled and working. If you choose to configure pg_audit via a postgresql.conf file, then you will need to define your own custom postgresql.conf file and mount it to override the default postgresql.conf file.</div><div class="section3"><a id="_container_examples/_pg_audit/_docker_15"></a><div class="section3-header"><div class="section3-number">2.20.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the following to create a database container:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgaudit
./run.sh</pre><div class="section-body-text">This starts an instance of the Audit container (running crunchy-postgres) on port 12005 on localhost. You can then run the test script as follows:</div><pre class="code-block">./test-pgaudit.sh</pre><div class="section-body-text">This test executes a SQL file which contains pg_audit configuration statements as well as executes some basic SQL commands. These SQL commands will cause pg_audit to create audit log messages in the pg_log log file created by the database container.</div></div></div><div class="section3"><a id="_container_examples/_pg_audit/_kubernetes_18"></a><div class="section3-header"><div class="section3-number">2.20.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Run the following:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgaudit
./run.sh</pre><div class="section-body-text">The script will create an Audit pod (running the crunchy-postgres container) on the Kubernetes instance and then executes a SQL file which contains pg_audit configuration statements as well as executes some basic SQL commands. These SQL commands will cause pg_audit to create audit log messages in the pg_log log file created by the database container.</div></div></div><div class="section3"><a id="_container_examples/_pg_audit/_openshift_17"></a><div class="section3-header"><div class="section3-number">2.20.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Run the following:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgaudit
./run.sh</pre><div class="section-body-text">The script will create an Audit pod (running the crunchy-postgres container) on the OpenShift instance and then executes a SQL file which contains pg_audit configuration statements as well as executes some basic SQL commands. These SQL commands will cause pg_audit to create audit log messages in the pg_log log file created by the database container.</div></div></div></div></div><div class="section2"><a id="_container_examples/_docker_swarm"></a><div class="section2-header"><div class="section2-number">2.21</div><div class="section2-title">Docker Swarm</div></div><div class="section-body"><div class="section-body-text">This example shows how to run a primary and replica database container on a Docker Swarm (v.1.12) cluster.</div><div class="section-body-text">First, set up a cluster. The Kubernetes libvirt coreos cluster example works well; see <a href="http://kubernetes.io/docs/getting-started-guides/libvirt-coreos/">coreos-libvirt-cluster.</a></div><div class="section-body-text">Next, on each node, create the Swarm using these <a href="https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/">Swarm Install instructions.</a></div><div class="section-body-text">Includes the command on the manager node:</div><pre class="code-block">docker swarm init --advertise-addr 192.168.10.1</pre><div class="section-body-text">Then the command on all the worker nodes:</div><pre class="code-block">docker swarm join \
    --token SWMTKN-1-65cn5wa1qv76l8l45uvlsbprogyhlprjpn27p1qxjwqmncn37o-015egopg4jhtbmlu04faon82u \
        192.168.10.1.37</pre><div class="section-body-text">Before creating Swarm services, for service discovery you need to define an overlay network to be used by the services you will create. Create the network like this:</div><pre class="code-block">docker network create --driver overlay crunchynet</pre><div class="section-body-text">We want to have the primary database always placed on a specific node. This is accomplished using node constraints as follows:</div><pre class="code-block">docker node inspect kubernetes-node-1 | grep ID
docker node update --label-add type=primary 18yrb7m650umx738rtevojpqy</pre><div class="section-body-text">In the above example, the kubernetes-node-1 node with ID 18yrb7m650umx738rtevojpqy has a user defined label of <b>primary</b> added to it. The primary service specifies <b>primary</b> as a constraint when created; this tells Swarm to place the service on that specific node. The replica specifies a constraint of <b>node.labels.type != primary</b> to have the replica always placed on a node that is not hosting the primary service.</div><div class="section3"><a id="_container_examples/_docker_swarm/_docker_16"></a><div class="section3-header"><div class="section3-number">2.21.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">After you set up the Swarm cluster, you can then run this example as follows on the <b>Swarm Manager Node</b>:</div><pre class="code-block">cd $CCPROOT/examples/docker/swarm-service
./run.sh</pre><div class="section-body-text">You can then find the nodes that are running the primary and replica containers by:</div><pre class="code-block">docker service ps primary
docker service ps replica</pre><div class="section-body-text">You can also scale up the number of <b>replica</b> containers.</div><pre class="code-block">docker service scale replica=2
docker service ls</pre><div class="section-body-text">Verify you have two replicas within PostgreSQL by viewing the <b>pg_stat_replication</b> table. The password is <b>password</b> by default when logged into the kubernetes-node-1 host:</div><pre class="code-block">docker exec -it $(docker ps -q) psql -U postgres -c 'table pg_stat_replication' postgres</pre><div class="section-body-text">You should see a row for each replica along with its replication status.</div></div></div></div></div><div class="section2"><a id="_container_examples/_pg_upgrade"></a><div class="section2-header"><div class="section2-number">2.22</div><div class="section2-title">pg_upgrade</div></div><div class="section-body"><div class="section-body-text">Starting in release 1.3.1, the upgrade container will let you perform a pg_upgrade on a 9.5 database converting its data to a 9.6 version.</div><div class="section-body-text">This example assumes you have run <b>primary-pvc</b> using a PG 9.5 image such as <b>centos7-9.5.12-1.8.1</b> prior to running this upgrade.</div><div class="section-body-text">Prior to starting this example, shut down the <b>primary-pvc</b> database using the <b>examples/kube/primary-pvc/cleanup.sh</b> script.</div><div class="section-body-text">Prior to running this example, make sure your CCP_IMAGE_TAG environment variable is using a PG 9.6 image such as <b>centos7-9.6.8-1.8.1</b>.</div><div class="section-body-text">This will create the following in your Kubernetes environment:</div><ul class="list-unordered"><li class="list-unordered">a Kubernetes Job running the <b>crunchy-upgrade</b> container</li><li class="list-unordered">a new data directory name <b>primary-upgrade</b> found in the <b>pgnewdata</b> PVC</li></ul><div class="section3"><a id="_container_examples/_pg_upgrade/_kubernetes_19"></a><div class="section3-header"><div class="section3-number">2.22.1</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Start the upgrade as follows:</div><pre class="code-block">cd $CCPROOT/examples/kube/upgrade
./run.sh</pre><div class="section-body-text">If successful, the Job will end with a Successful status. Verify the results of the Job by examining the Job's pod log:</div><pre class="code-block">kubectl get pod -a -l job-name=upgrade-job
kubectl logs -l job-name=upgrade-job</pre><div class="section-body-text">You can verify the upgraded database by running the <b>examples/kube/primary-upgrade</b> example. This example will mount the newly created and upgraded database files. Database tables and data that were in the <b>primary-pvc</b> test database should be found in the <b>primary-upgrade</b> database.</div></div></div></div></div><div class="section2"><a id="_container_examples/_performing_a_pg_dump"></a><div class="section2-header"><div class="section2-number">2.23</div><div class="section2-title">Performing a pg_dump</div></div><div class="section-body"><div class="section-body-text">The script assumes you are going to backup the <b>basic</b> container created in the first example, so you need to ensure that container is running. This example assumes you have configured NFS as described in the <a href="install.adoc">installation documentation</a>. Things to point out with this example include its use of persistent volumes and volume claims to store the backup data files to an NFS server.</div><div class="section-body-text">A successful backup will perform pg_dump/pg_dumpall on the basic and store the resulting files in the NFS mounted volume under a directory named using the database host name plus -dumps as a sub-directory, then followed by a unique backup directory based upon a date/timestamp - allowing any number of backups to be kept.</div><div class="section-body-text">The dump script will do the following:</div><ul class="list-unordered"><li class="list-unordered">Start up a backup container named pgdump-job</li><li class="list-unordered">Run pg_dump/pg_dumpall on the container named basic</li><li class="list-unordered">Store the backup in the PV in a path named with a date/timestamp</li><li class="list-unordered">Exit after the backup</li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_performing_a_pg_dump/_docker_17"></a><div class="section3-header"><div class="section3-number">2.23.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the backup with this command:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgdump
./run.sh

#Output from the run.sh script will include output like:
#starting backup container...
#Cleaning up...
#pgdump
#pgdump
#pgdump-volume
#pgdump-volume
#0367ee9cbe776450973f63ab875ab868c3aa8902ec3e073de8adbab9baa75c14

#Make note of the container ID from above to run the following command
docker logs 0367ee9cbe776450973f63ab875ab868c3aa8902ec3e073de8adbab9baa75c14 2>&amp;1 | grep "output"

#That will return the location where the pg_dump/pg_dumpall file(s) were written.  E.g.:
#PGDUMP_ALL output file has been written to: /pgdata/basic-dumps/2018-02-14-05-00-23/pgdumpall.sql

#Make note of the timestamp above and run a find to get the fully-qualified filesystem path where the file was written.  E.g.:
sudo find / -name 2018-02-14-05-00-23

#That will return the fully-qualified path, where the file can be accessed/copied to your local filesystem.  E.g.:
#/var/lib/docker/volumes/pgdump-volume/_data/basic-dumps/2018-02-14-05-00-23

#Copy the file (path returned above + the filename) to your local filesystem for use or for running with the pg_restore container:
sudo cp -p /var/lib/docker/volumes/pgdump-volume/_data/basic-dumps/2018-02-14-05-00-23/pgdumpall.sql /tmp</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_pg_dump/_kubernetes_20"></a><div class="section3-header"><div class="section3-number">2.23.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgdump-job
./run.sh</pre><div class="section-body-text">The Kubernetes Job type executes a pod and then the pod exits. You can view the Job status using this command:</div><pre class="code-block">kubectl get job</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_pg_dump/_openshift_18"></a><div class="section3-header"><div class="section3-number">2.23.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Start the backup:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgdump-job
./run.sh</pre><div class="section-body-text">The <b>pgdump-job.json</b> file within that directory specifies options that control the behavior of the pgdump-job. E.g. Whether to run pg_dump vs pg_dumpall, whether to include verbose output, if database objects should be cleanly dropped before being recreated, etc.</div></div></div></div></div><div class="section2"><a id="_container_examples/_performing_a_pg_restore"></a><div class="section2-header"><div class="section2-number">2.24</div><div class="section2-title">Performing a pg_restore</div></div><div class="section-body"><div class="section-body-text">The script assumes you are going to restore to the <b>basic</b> container created in the first example, so you need to ensure that container is running. This example assumes you have configured NFS as described in the <a href="install.adoc">installation documentation</a>. Things to point out with this example include its use of persistent volumes and volume claims to store the backup data files to an NFS server.</div><div class="section-body-text">Successful use of the crunchy-restore container will run a job to restore files generated by pg_dump/pg_dumpall to a container via psql/pg_restore; then container will terminate successfully and signal job completion.</div><div class="section-body-text">The restore script will do the following:</div><ul class="list-unordered"><li class="list-unordered">Mount a PV/PVC as named in the JSON file</li><li class="list-unordered">Run psql/pg_restore on the container named basic (or as specified otherwise in the JSON file)</li><li class="list-unordered">Exit after the backup</li></ul><div class="section-body-text">To shutdown the instance and remove the container for each example, run the following:</div><pre class="code-block">./cleanup.sh</pre><div class="section3"><a id="_container_examples/_performing_a_pg_restore/_docker_18"></a><div class="section3-header"><div class="section3-number">2.24.1</div><div class="section3-title">Docker</div></div><div class="section-body"><div class="section-body-text">Run the backup with this command:</div><pre class="code-block">cd $CCPROOT/examples/docker/pgrestore
./run.sh</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_pg_restore/_kubernetes_21"></a><div class="section3-header"><div class="section3-number">2.24.2</div><div class="section3-title">Kubernetes</div></div><div class="section-body"><div class="section-body-text">Running the example:</div><pre class="code-block">cd $CCPROOT/examples/kube/pgrestore-job
./run.sh</pre><div class="section-body-text">The Kubernetes Job type executes a pod and then the pod exits. You can view the Job status using this command:</div><pre class="code-block">kubectl get job</pre></div></div><div class="section3"><a id="_container_examples/_performing_a_pg_restore/_openshift_19"></a><div class="section3-header"><div class="section3-number">2.24.3</div><div class="section3-title">OpenShift</div></div><div class="section-body"><div class="section-body-text">Start the restore:</div><pre class="code-block">cd $CCPROOT/examples/openshift/pgrestore-job
./run.sh</pre><div class="section-body-text">The <b>pgrestore-job.json</b> file within that directory specifies options that control the behavior of the pgrestore-job. E.g. Whether to restore via psql vs pg_restore (dependent on the PGRESTORE_FORMAT variable), whether to include verbose output, if database objects should be cleanly dropped before being recreated, etc.</div></div></div><div class="section3"><a id="_container_examples/_performing_a_pg_restore/_sshd_postgresql"></a><div class="section3-header"><div class="section3-number">2.24.4</div><div class="section3-title">SSHD PostgreSQL</div></div><div class="section-body"><div class="section-body-text">To enable SSHD on PostgreSQL, see the <a href="sshd.adoc">SSHD Documentation</a>.</div></div></div></div></div></div></div><div class="section1"><a id="_legal_notices"></a><div class="section1-header"><div class="section1-number">3</div><div class="section1-title">Legal Notices</div></div><div class="section-body"><div class="section-body-text">Copyright &copy; 2018 Crunchy Data Solutions, Inc.</div><div class="section-body-text">CRUNCHY DATA SOLUTIONS, INC. PROVIDES THIS GUIDE <q>AS IS</q> WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.</div><div class="section-body-text">Crunchy, Crunchy Data Solutions, Inc. and the Crunchy Hippo Logo are trademarks of Crunchy Data Solutions, Inc.</div></div></div><div class="page-footer text-center">Prepared by Crunchy Data Solutions, Inc. &mdash; March 22, 2018</div></div></div></div></body></html>